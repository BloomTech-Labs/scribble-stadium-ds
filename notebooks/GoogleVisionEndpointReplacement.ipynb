{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GoogleVisionEndpointReplacement.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_zoKxm5K_xF"
      },
      "source": [
        "# Tesseract toGoogle Vision Endpoints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wyl6-W33LRU9"
      },
      "source": [
        "### Stake holders would like a product that eventually no longer utalizes google vision for text image processessing. \n",
        "This note book is set up to provide documentation on the StorySquadApp as to where end points need to be replaced with our own models. As more is done to craft the model and the preprocessing steps more hard code can be added to modulate into the codebase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDjcCDJNYg91"
      },
      "source": [
        "From app/api/submission.py in the submission_text function the google API is called to transcribe the text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Svc9MmhdYggG"
      },
      "source": [
        "### Google Vision Endpoint at app/api/sumission\n",
        "async def submission_text(sub: Submission):\n",
        "    \"\"\"Takes a Submission Object and calls the Google Vision API to text annotate\n",
        "    the passed s3 link, then passes those concatenated transcriptions to the SquadScore\n",
        "    method, returns:\n",
        "\n",
        "    Arguments:\n",
        "    ---\n",
        "    `sub`: Submission - Submission object **see `help(Submission)` for more info**\n",
        "    Returns:\n",
        "    ---\n",
        "    ```\n",
        "    {\"SubmissionID\": int, \"IsFlagged\": boolean,\"LowConfidence\": boolean, \"Complexity\": int}\n",
        "    ```\n",
        "    \"\"\"\n",
        "    transcriptions = \"\"\n",
        "    confidence_flags = []\n",
        "    # unpack links for files in submission object\n",
        "    for page_num in sub.Pages:\n",
        "        # re-init the sha algorithm every file that is processed\n",
        "        hash = sha512()\n",
        "        # fetch file from s3 bucket\n",
        "        r = get(sub.Pages[page_num][\"URL\"])\n",
        "        # update the hash with the file's content\n",
        "        hash.update(r.content)\n",
        "        try:\n",
        "            # assert that the hash is the same as the one passed with the file\n",
        "            # link\n",
        "            assert hash.hexdigest() == sub.Pages[page_num][\"Checksum\"]\n",
        "        except AssertionError:\n",
        "            # return some useful information about the error including what\n",
        "            # caused it and the file affected\n",
        "            return JSONResponse(\n",
        "                status_code=422,\n",
        "                content={\"ERROR\": \"BAD CHECKSUM\", \"file\": sub.Pages[page_num]},\n",
        "            )\n",
        "        # unpack response from GoogleAPI\n",
        "        conf_flag, flagged, trans = await vision.transcribe(r.content)\n",
        "        # concat transcriptions togeather\n",
        "        transcriptions += trans + \"\\n\"\n",
        "        # add page to list of confidence flags\n",
        "        confidence_flags.append(conf_flag)\n",
        "    # score the transcription using SquadScore algorithm\n",
        "    score = await squad_score(transcriptions, scaler)\n",
        "\n",
        "    # return the complexity score to the web team with the SubmissionID\n",
        "    return JSONResponse(\n",
        "        status_code=200,\n",
        "        content={\n",
        "            \"SubmissionID\": sub.SubmissionID,\n",
        "            \"IsFlagged\": flagged,\n",
        "            \"LowConfidence\": True in confidence_flags,\n",
        "            \"Complexity\": score,\n",
        "        },\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jd74FuFHhy50"
      },
      "source": [
        "From app/utils/img_processing/confidence_flag.py\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vk4qu74fKa6C"
      },
      "source": [
        "def image_confidence(image_path):\n",
        "    \"\"\"\n",
        "    Detects text in images and calculates the confidence level for each\n",
        "    character. Returns a True boolean if the overall confidence for the\n",
        "    page is less than 0.85. Otherwise, returns False\n",
        "\n",
        "        Input: Path to file where image is stored\n",
        "            One image per call: run function on each image in a submission\n",
        "        Output: Boolean; True if confidence level for page is less than 0.85\n",
        "                False if confidence is 0.85 or greater\n",
        "    \"\"\"\n",
        "\n",
        "    # If image_path is local\n",
        "    with io.open(image_path, \"rb\") as image_file:\n",
        "        content = image_file.read()\n",
        "    image = vision.types.Image(content=content)\n",
        "\n",
        "    # # If image_path is a uri\n",
        "    # image = vision.types.Image()\n",
        "    # image.source.image_uri = uri\n",
        "\n",
        "    # Set language to english only\n",
        "    language = vision.types.ImageContext(language_hints=[\"en-t-i0-handwrit\"])\n",
        "\n",
        "    # Connect to Google API client\n",
        "    creds = service_account.Credentials.from_service_account_file(\n",
        "        \"/Users/stevenchase/Desktop/Steven/Computer_Science/Lambda/labs/story_sqaud/Story Squad-6122da7459cf.json\"\n",
        "    )\n",
        "    client = vision.ImageAnnotatorClient(credentials=creds)\n",
        "    response = client.document_text_detection(\n",
        "        image=image, image_context=language\n",
        "    )\n",
        "\n",
        "    # List of confidence levels of each character\n",
        "    symbol_confidences = []\n",
        "\n",
        "    for page in response.full_text_annotation.pages:\n",
        "        for block in page.blocks:\n",
        "            for paragraph in block.paragraphs:\n",
        "                for word in paragraph.words:\n",
        "                    for symbol in word.symbols:\n",
        "                        symbol_confidences.append(symbol.confidence)\n",
        "\n",
        "    # If there is no text on the page\n",
        "    if len(symbol_confidences) == 0:\n",
        "        return \"No Text Detected\"\n",
        "    else:\n",
        "        # Calculate the overall confidence for the page\n",
        "        page_confidence = sum(symbol_confidences) / len(symbol_confidences)\n",
        "\n",
        "        # Return flag: True under 85% confident, False 85% confident or over\n",
        "        if page_confidence < 0.85:\n",
        "            return True\n",
        "        else:\n",
        "            return False\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}