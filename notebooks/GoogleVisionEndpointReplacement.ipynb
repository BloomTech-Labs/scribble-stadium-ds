{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GoogleVisionEndpointReplacement.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_zoKxm5K_xF"
      },
      "source": [
        "# Tesseract toGoogle Vision Endpoints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wyl6-W33LRU9"
      },
      "source": [
        "### Stake holders would like a product that eventually no longer utalizes google vision for text image processessing. \n",
        "This note book is set up to provide documentation on the StorySquadApp as to where end points need to be replaced with our own models. As more is done to craft the model and the preprocessing steps more hard code can be added to modulate into the codebase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDjcCDJNYg91"
      },
      "source": [
        "From app/api/submission.py in the submission_text function the google API is called to transcribe the text\n",
        "\n",
        "The google cloud vision api is called around line 55 (conf_flag, flagged, trans = await vision.transcribe(r.content)), this should be replaced with Tesseract.transcribe() which is a function located in Tesseract.py in the Tessie_True repo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Svc9MmhdYggG"
      },
      "source": [
        "### Google Vision Endpoint at app/api/submission\n",
        "async def submission_text(sub: Submission):\n",
        "    \"\"\"Takes a Submission Object and calls the Google Vision API to text annotate\n",
        "    the passed s3 link, then passes those concatenated transcriptions to the SquadScore\n",
        "    method, returns:\n",
        "\n",
        "    Arguments:\n",
        "    ---\n",
        "    `sub`: Submission - Submission object **see `help(Submission)` for more info**\n",
        "    Returns:\n",
        "    ---\n",
        "    ```\n",
        "    {\"SubmissionID\": int, \"IsFlagged\": boolean,\"LowConfidence\": boolean, \"Complexity\": int}\n",
        "    ```\n",
        "    \"\"\"\n",
        "    transcriptions = \"\"\n",
        "    confidence_flags = []\n",
        "    # unpack links for files in submission object\n",
        "    for page_num in sub.Pages:\n",
        "        # re-init the sha algorithm every file that is processed\n",
        "        hash = sha512()\n",
        "        # fetch file from s3 bucket\n",
        "        r = get(sub.Pages[page_num][\"URL\"])\n",
        "        # update the hash with the file's content\n",
        "        hash.update(r.content)\n",
        "        try:\n",
        "            # assert that the hash is the same as the one passed with the file\n",
        "            # link\n",
        "            assert hash.hexdigest() == sub.Pages[page_num][\"Checksum\"]\n",
        "        except AssertionError:\n",
        "            # return some useful information about the error including what\n",
        "            # caused it and the file affected\n",
        "            return JSONResponse(\n",
        "                status_code=422,\n",
        "                content={\"ERROR\": \"BAD CHECKSUM\", \"file\": sub.Pages[page_num]},\n",
        "            )\n",
        "        # unpack response from GoogleAPI\n",
        "        conf_flag, flagged, trans = await vision.transcribe(r.content)\n",
        "        # concat transcriptions togeather\n",
        "        transcriptions += trans + \"\\n\"\n",
        "        # add page to list of confidence flags\n",
        "        confidence_flags.append(conf_flag)\n",
        "    # score the transcription using SquadScore algorithm\n",
        "    score = await squad_score(transcriptions, scaler)\n",
        "\n",
        "    # return the complexity score to the web team with the SubmissionID\n",
        "    return JSONResponse(\n",
        "        status_code=200,\n",
        "        content={\n",
        "            \"SubmissionID\": sub.SubmissionID,\n",
        "            \"IsFlagged\": flagged,\n",
        "            \"LowConfidence\": True in confidence_flags,\n",
        "            \"Complexity\": score,\n",
        "        },\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jd74FuFHhy50"
      },
      "source": [
        "From app/utils/img_processing/confidence_flag.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vk4qu74fKa6C"
      },
      "source": [
        "def image_confidence(image_path):\n",
        "    \"\"\"\n",
        "    Detects text in images and calculates the confidence level for each\n",
        "    character. Returns a True boolean if the overall confidence for the\n",
        "    page is less than 0.85. Otherwise, returns False\n",
        "\n",
        "        Input: Path to file where image is stored\n",
        "            One image per call: run function on each image in a submission\n",
        "        Output: Boolean; True if confidence level for page is less than 0.85\n",
        "                False if confidence is 0.85 or greater\n",
        "    \"\"\"\n",
        "\n",
        "    # If image_path is local\n",
        "    with io.open(image_path, \"rb\") as image_file:\n",
        "        content = image_file.read()\n",
        "    image = vision.types.Image(content=content)\n",
        "\n",
        "    # # If image_path is a uri\n",
        "    # image = vision.types.Image()\n",
        "    # image.source.image_uri = uri\n",
        "\n",
        "    # Set language to english only\n",
        "    language = vision.types.ImageContext(language_hints=[\"en-t-i0-handwrit\"])\n",
        "\n",
        "    # Connect to Google API client\n",
        "    creds = service_account.Credentials.from_service_account_file(\n",
        "        \"/Users/stevenchase/Desktop/Steven/Computer_Science/Lambda/labs/story_sqaud/Story Squad-6122da7459cf.json\"\n",
        "    )\n",
        "    client = vision.ImageAnnotatorClient(credentials=creds)\n",
        "    response = client.document_text_detection(\n",
        "        image=image, image_context=language\n",
        "    )\n",
        "\n",
        "    # List of confidence levels of each character\n",
        "    symbol_confidences = []\n",
        "\n",
        "    for page in response.full_text_annotation.pages:\n",
        "        for block in page.blocks:\n",
        "            for paragraph in block.paragraphs:\n",
        "                for word in paragraph.words:\n",
        "                    for symbol in word.symbols:\n",
        "                        symbol_confidences.append(symbol.confidence)\n",
        "\n",
        "    # If there is no text on the page\n",
        "    if len(symbol_confidences) == 0:\n",
        "        return \"No Text Detected\"\n",
        "    else:\n",
        "        # Calculate the overall confidence for the page\n",
        "        page_confidence = sum(symbol_confidences) / len(symbol_confidences)\n",
        "\n",
        "        # Return flag: True under 85% confident, False 85% confident or over\n",
        "        if page_confidence < 0.85:\n",
        "            return True\n",
        "        else:\n",
        "            return False\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "From app/utils/img_processing/safe_search.py"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "## Thoughts on safe search feature from stakeholders\n",
        "* The safe search feature is reuqired for the app as its target audience is children and this will help prevent explicit content from being present\n",
        "* Right now, tesseract does not have a safe search feature implemented so this endpoint should not be switched with tesseract\n",
        "* There may be some other open source options to replace this, but as of now Google Cloud Vision is probably the most well trained for explicit content detection in images\n",
        "* The fee structure for Google Cloud Vision is payed for each feature such that, according to Google \"Each feature applied to an image is a billable unit. For example, if you apply Face Detection and Label Detection to the same image, you are billed for one unit of Label Detection and one unit for Face Detection.\" With the goal of minimizing costs for the stakeholders, only applying the Google Could Vision API for one feature should not be too extreme of a cost for a feature that is needed for the app\n",
        "* More information about the fee structure can be found at https://cloud.google.com/vision/pricing"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Connect to Google Cloud Vision API and utilize their safe_search to\n",
        "# moderate illustration submissions\n",
        "\n",
        "from google.cloud import vision\n",
        "import io\n",
        "\n",
        "\n",
        "def detect_safe_search(path):\n",
        "    \"\"\"\n",
        "    Detects adult, violent or racy content in uploaded images\n",
        "        Input: path to the image file\n",
        "        Output: String, either stating 'No inappropriate material detected'\n",
        "            or 'Image Flagged' with information about what is inappropriate\n",
        "    \"\"\"\n",
        "\n",
        "    client = vision.ImageAnnotatorClient()\n",
        "\n",
        "    # If local illustration\n",
        "    with io.open(path, \"rb\") as image_file:\n",
        "        content = image_file.read()\n",
        "    image = vision.types.Image(content=content)\n",
        "\n",
        "    # # If remote illustration\n",
        "    # image = vision.types.Image()\n",
        "    # image.source.image_uri = uri\n",
        "\n",
        "    response = client.safe_search_detection(image=image)\n",
        "    safe = response.safe_search_annotation\n",
        "\n",
        "    # Names of likelihood from google.cloud.vision.enums\n",
        "    likelihood_name = (\n",
        "        \"UNKNOWN\",\n",
        "        \"VERY_UNLIKELY\",\n",
        "        \"UNLIKELY\",\n",
        "        \"POSSIBLE\",\n",
        "        \"LIKELY\",\n",
        "        \"VERY_LIKELY\",\n",
        "    )\n",
        "\n",
        "    # Check illustration against each safe_search category\n",
        "    # Flag if inappropriate material is 'Possible' or above\n",
        "    if safe.adult > 2 or safe.violence > 2 or safe.racy > 2:\n",
        "        # Set flag - provide information about what is inappropriate\n",
        "        flagged = [\n",
        "            (\"adult: {}\".format(likelihood_name[safe.adult])),\n",
        "            (\"violence: {}\".format(likelihood_name[safe.violence])),\n",
        "            (\"racy: {}\".format(likelihood_name[safe.racy])),\n",
        "        ]\n",
        "        return f\"Image Flagged: {flagged}\"\n",
        "\n",
        "    else:\n",
        "        return \"No inappropriate material detected\""
      ]
    },
    {
      "source": [
        "From app/utils/img_processing/transcription.py\n",
        "\n",
        "This is no longer needed as tesseract will be used for text transcription"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Google Vision function to extract text from a local or uri hosted image\n",
        "\n",
        "from google.cloud import vision\n",
        "import io\n",
        "\n",
        "\n",
        "def transcribe(image_path):\n",
        "    \"\"\"\n",
        "    Detects document features in images and returns extracted text\n",
        "    Input: Path to file where images are stored\n",
        "        - Assuming 1 image per image_path\n",
        "        - Code for both local image_path and remote image_path, comment out\n",
        "            the apporopriate one\n",
        "    Output: Transcribed text as a string\n",
        "    \"\"\"\n",
        "\n",
        "    # If image_path is local\n",
        "    with io.open(image_path, \"rb\") as image_file:\n",
        "        content = image_file.read()\n",
        "    image = vision.types.Image(content=content)\n",
        "\n",
        "    # # If image_path is a uri\n",
        "    # image = vision.types.Image()\n",
        "    # image.source.image_uri = uri\n",
        "\n",
        "    # Connect to Google API client\n",
        "    client = vision.ImageAnnotatorClient()\n",
        "    response = client.document_text_detection(image=image)\n",
        "\n",
        "    # Save transcribed text\n",
        "    if response.text_annotations:\n",
        "        transcribed_text = response.text_annotations[0].description.replace(\n",
        "            \"\\n\", \" \"\n",
        "        )\n",
        "    else:\n",
        "        print(\"No Text Detected\")\n",
        "\n",
        "    return transcribed_text"
      ]
    }
  ]
}